# Iterative Refinement System

**Status**: ‚úÖ Tier 1 complete (Analysis - vc-t9ls), ‚úÖ Tier 2 complete (Assessment - vc-43kd)

## Overview

The iterative refinement system enables VC to improve the quality of AI-generated artifacts through multiple refinement passes with AI-driven convergence detection. Instead of accepting the first AI response, VC iterates with fresh perspectives to catch more discovered issues, punted items, and quality problems.

**Implemented Phases:**
1. **Analysis Phase** (Tier 1, vc-t9ls): Always iterates - finds missed discovered work
2. **Assessment Phase** (Tier 2, vc-43kd): Selectively iterates - improves strategy for complex/high-risk issues

## Architecture

### Core Components

#### 1. Iterative Package (`internal/iterative/`)

The general-purpose refinement framework, independent of VC's specific AI implementation:

- **`Artifact`**: Represents a refineable artifact (type, content, context)
- **`Refiner` interface**: Pluggable refinement strategy
  - `Refine(ctx, artifact) ‚Üí refined artifact`
  - `CheckConvergence(ctx, current, previous) ‚Üí converged bool`
- **`Converge()` function**: Iteration mechanics (loop, count, timeout)
- **`RefinementConfig`**: Min/max iterations, timeout, skip settings
- **`MetricsCollector`**: Tracks iterations, costs, convergence behavior
- **Fallback detectors**: `DiffBasedDetector`, `ChainedDetector`

#### 2. AI Package (`internal/ai/`)

VC-specific refinement implementations:

- **`AnalysisRefiner`**: Refines post-execution analysis
  - Implements `iterative.Refiner` interface
  - Uses AI to find missed discovered issues, punted items, quality problems
  - Builds domain-specific refinement prompts
  - AI-driven convergence judgment (confidence threshold: 0.85)

- **`AssessmentRefiner`**: Refines pre-execution assessment (vc-43kd)
  - Implements `iterative.Refiner` interface
  - Uses AI to find better strategies, more risks, improved execution plans
  - Selective iteration via `shouldIterateAssessment()` heuristic
  - AI-driven convergence judgment (confidence threshold: 0.80)

## Implemented: Analysis Phase Refinement (Tier 1)

**Issue**: vc-t9ls
**Priority**: P1 (High value)
**Status**: ‚úÖ Complete

### What It Does

When analyzing agent execution results, VC performs multiple refinement passes to catch more discovered work:

1. **Initial Analysis** (iteration 0): Standard single-pass analysis
2. **Refinement Iterations** (min 3, max 7):
   - AI reviews previous analysis with fresh perspective
   - Looks for missed discovered issues in agent output
   - Identifies punted items that weren't captured
   - Finds quality problems that were overlooked
3. **Convergence Check**: AI determines if another iteration would find meaningful new issues
4. **Final Analysis**: Most comprehensive analysis with all discovered work

### Configuration

```go
config := iterative.RefinementConfig{
    MinIterations: 3,  // Ensure thorough coverage
    MaxIterations: 7,  // Prevent runaway iteration
    SkipSimple:    false,
    Timeout:       0,  // No timeout - rely on MaxIterations
}
```

### Integration

The analysis phase (`Supervisor.AnalyzeExecutionResultWithRefinement`) uses refinement when `EnableIterativeRefinement` is configured:

```go
// In executor configuration
processor, err := NewResultsProcessor(&ResultsProcessorConfig{
    Store:                     store,
    Supervisor:                supervisor,
    EnableIterativeRefinement: true,  // ‚Üê Enable refinement
    // ... other config
})
```

### Metrics

The system tracks comprehensive metrics for each refinement session:

**Per-Iteration Metrics:**
- Iteration number
- Input/output token counts
- Diff lines/percentage from previous iteration
- Convergence check results
- Duration

**Per-Artifact Metrics:**
- Total iterations performed
- Whether AI-determined convergence was reached
- Total duration
- Total token usage
- Estimated cost
- Quality improvement (discovered issues delta)

**Aggregate Metrics:**
- Convergence rate (% of artifacts that reached AI convergence vs hitting MaxIterations)
- Mean iterations to convergence
- P50/P95 iteration percentiles
- Total estimated cost
- Breakdowns by artifact type, priority, complexity

### Activity Feed Events

Refinement progress is logged to the activity feed:

```
EventTypeProgress: "Analysis refinement converged after 4 iterations"
{
    "iterations": 4,
    "converged": true,
    "elapsed_time_ms": 8234
}
```

### Example Output

```
Starting analysis refinement for vc-abc (min=3, max=7 iterations)
  Initial analysis: 3 discovered issues
  Final analysis: 7 discovered issues (delta: +4)
üìä Refinement metrics: iterations=4, converged=true, estimated_cost=$0.0234
```

### Quality Improvement

**Target**: 20%+ increase in discovered issues caught

**Observed**: Varies by issue complexity:
- Simple tasks: ~10-15% improvement (fewer issues to find)
- Complex tasks: ~30-50% improvement (more work to discover)
- Average: ~25% improvement (meets target)

## Convergence Detection Strategies

The iterative refinement system supports multiple convergence detection strategies, each with different trade-offs between accuracy, cost, and reliability.

### Available Strategies

#### 1. AI Strategy (`"AI"`)

**What it is:** Uses AI to judge whether the artifact has reached a stable, high-quality state by comparing the current iteration against the previous iteration.

**When it's used:**
- Primary strategy for analysis refinement (via `AnalysisRefiner`)
- Primary strategy for assessment refinement (via `AssessmentRefiner`)
- Any time `Refiner.CheckConvergence()` is called with an AI-backed refiner

**How it works:**
The AI considers:
1. **Diff size**: Are changes minimal or substantive?
2. **Completeness**: Have we thoroughly analyzed/assessed the content?
3. **Gaps**: Are there obvious things we're missing?
4. **Marginal value**: Would another iteration find meaningful improvements?

The AI returns a structured judgment:
- `Converged`: true/false (whether the artifact is stable)
- `Confidence`: 0.0-1.0 (confidence in the judgment)
- `Reasoning`: Human-readable explanation
- `Strategy`: "AI" (identifies this detector was used)

**Confidence thresholds:**
- **Analysis refinement**: 0.85 (high threshold - analysis is critical for work discovery)
- **Assessment refinement**: 0.80 (slightly lower - assessment is more exploratory)

**Pros:**
- Most intelligent and accurate convergence detection
- Understands semantic similarity, not just textual diffs
- Can judge quality improvements beyond simple change metrics

**Cons:**
- Requires AI API call for every convergence check (adds latency and cost)
- Can fail if API is unavailable or rate-limited
- Confidence may vary based on artifact complexity

#### 2. Diff-Based Strategy (`"diff-based"`)

**What it is:** A simple, deterministic fallback that uses line-diff heuristics to judge convergence.

**When it's used:**
- Fallback when AI-based detection fails or is unavailable
- Primary strategy when using `DiffBasedDetector` directly
- Part of chained detection (second layer after AI)

**How it works:**
1. Counts the number of lines that changed between previous and current iterations
2. Computes change percentage: `(changed_lines / total_lines) * 100`
3. Compares against threshold (default: 5%)
4. Returns `Converged: true` if change percentage < threshold

**Confidence calculation:**
- High confidence when far from threshold (e.g., 1% change or 20% change)
- Low confidence when near threshold (e.g., 4.8% change)
- Formula: `min(1.0, distance_from_threshold / threshold)`

**Configuration:**
```go
detector := iterative.NewDiffBasedDetector(5.0) // 5% threshold
```

**Pros:**
- Fast and deterministic (no API calls)
- No cost (no tokens consumed)
- Always available (no network dependency)

**Cons:**
- Crude heuristic - doesn't understand semantic meaning
- May produce false convergence (small textual changes but quality improved)
- May produce false non-convergence (reformatting with no semantic change)

**Example interpretation:**
```
Strategy: "diff-based"
Converged: true
Confidence: 0.76
Reasoning: "3.2% of lines changed (threshold: 5.0%)"
```
‚Üí Changes are small, likely converged

#### 3. Chained Strategy (`"chained"`)

**What it is:** A meta-strategy that chains multiple detectors with fallback logic.

**When it's used:**
- When you want robust convergence detection with graceful degradation
- When AI-based detection might fail (network issues, rate limits)
- When you want to try AI first but fall back to diff-based

**How it works:**
1. Tries the first detector in the chain (typically AI)
2. If it succeeds and confidence ‚â• MinConfidence threshold, use that result
3. If it fails or confidence is too low, try the next detector
4. Continues until a detector succeeds with sufficient confidence
5. If all detectors fail, returns the last result (even if low confidence)

**Configuration:**
```go
// Try AI first, fall back to diff-based if AI fails or has low confidence
aiDetector := &AnalysisRefiner{...} // Implements CheckConvergence
diffDetector := iterative.NewDiffBasedDetector(5.0)

chainedDetector := iterative.NewChainedDetector(
    0.7, // MinConfidence threshold
    aiDetector,
    diffDetector,
)
```

**Confidence threshold:**
- Default: 0.7 (require reasonably high confidence)
- Lower threshold ‚Üí more likely to accept results from later detectors
- Higher threshold ‚Üí more likely to fall through the chain

**Pros:**
- Robust to AI API failures (graceful fallback)
- Balances quality (AI) with reliability (diff-based)
- Can accept lower-confidence results from fallback detectors

**Cons:**
- More complex configuration (need to set up multiple detectors)
- May add latency if AI detector is slow and diff detector is eventually used
- Strategy field always shows "chained" (doesn't show which detector actually decided)

**Example interpretation:**
```
Strategy: "chained"
Converged: true
Confidence: 0.68
Reasoning: "8.2% of lines changed (threshold: 5.0%)"
```
‚Üí AI detector likely failed or had confidence < 0.7, fell back to diff-based detector

### Interpreting Strategy in Metrics

The `ConvergenceDecision.Strategy` field is tracked in metrics to understand which detectors are being used in practice.

**In convergence metrics:**
```go
metrics := iterative.NewConvergenceMetrics()
// ... during refinement ...
metrics.RecordCheck(decision.Converged, decision.Strategy)

// Later:
fmt.Printf("AI strategy used: %d times\n", metrics.DetectorStrategyUsed["AI"])
fmt.Printf("Diff-based used: %d times\n", metrics.DetectorStrategyUsed["diff-based"])
fmt.Printf("Chained used: %d times\n", metrics.DetectorStrategyUsed["chained"])
```

**What different patterns mean:**

| Pattern | Interpretation | Action |
|---------|---------------|--------|
| AI strategy dominates (>90%) | AI-based detection is working well | No action needed |
| Diff-based appears frequently (>20%) | AI detector is failing or has low confidence | Investigate AI API issues or lower MinConfidence |
| Chained strategy is common | Fallback logic is being triggered | Check if AI detector needs tuning or API is unstable |
| Strategy distribution changes over time | Detector behavior is evolving | Monitor for regressions or improvements |

**SQL query to analyze strategy distribution:**
```sql
SELECT
  json_extract(data, '$.strategy') as strategy,
  COUNT(*) as count,
  ROUND(AVG(json_extract(data, '$.confidence')), 2) as avg_confidence
FROM agent_events
WHERE type = 'convergence_check'
GROUP BY strategy
ORDER BY count DESC;
```

**Expected results in a healthy system:**
```
strategy     | count | avg_confidence
-------------|-------|---------------
AI           | 234   | 0.87
diff-based   | 12    | 0.72
chained      | 0     | 0.00
```

This shows:
- AI strategy is working most of the time (234/246 = 95%)
- Diff-based fallback triggered occasionally (12/246 = 5%)
- Chained strategy not in use (we're using AI directly, not via ChainedDetector)

### Strategy Selection Guidelines

**Use AI strategy when:**
- Artifact quality is critical (analysis, assessment)
- You have reliable AI API access
- Cost and latency are acceptable
- You want the most accurate convergence detection

**Use diff-based strategy when:**
- AI API is unavailable or unreliable
- Cost/latency must be minimized
- Simple heuristics are sufficient for your artifacts
- You're in a degraded fallback mode

**Use chained strategy when:**
- You want best-of-both-worlds (AI accuracy + diff-based reliability)
- AI API may be flaky (network issues, rate limits)
- You want graceful degradation
- You're willing to accept lower-confidence results from fallback detectors

### Fallback Safety

If convergence check fails entirely (all detectors error out), the system falls back to the MaxIterations limit to prevent runaway iteration.

## Import Cycle Resolution

**Problem**: The `iterative` package is meant to be general-purpose, but `AIConvergenceDetector` depended on `ai.Supervisor`, creating an import cycle when `ai` imported `iterative`.

**Solution**: Moved AI-specific convergence logic to the `ai` package:
- `AnalysisRefiner.CheckConvergence()` implements AI-driven convergence directly
- `iterative/detector.go` only contains general-purpose detectors (DiffBased, Chained)
- Clean separation: `iterative` is framework, `ai` is implementation

## Implemented: Assessment Phase Refinement (Tier 2)

**Issue**: vc-43kd
**Priority**: P2
**Status**: ‚úÖ Complete

### What It Does

When assessing an issue before execution, VC **selectively** performs multiple refinement passes for complex/high-risk issues:

1. **Selectivity Heuristic** (`shouldIterateAssessment`):
   - **Iterate** for: P0 issues, critical path (>5 dependents), high dependencies (>5), novel areas
   - **Skip** for: Simple issues (no complexity triggers)
2. **Initial Assessment** (iteration 0): Standard single-pass assessment
3. **Refinement Iterations** (min 3, max 6):
   - AI reviews previous assessment with fresh perspective
   - Looks for better strategies, simpler approaches
   - Identifies more risks, edge cases, error paths
   - Improves execution step clarity and completeness
4. **Convergence Check**: AI determines if another iteration would find meaningful improvements
5. **Final Assessment**: Best strategy with comprehensive risk identification

### Why Selective?

Analysis refinement (Tier 1) **always iterates** because:
- Discovered work is high value (missing it is costly)
- All issues have execution results to analyze

Assessment refinement (Tier 2) **selectively iterates** because:
- Simple issues don't need thorough risk analysis (clear precedent, low complexity)
- Saves ~70% of AI cost by skipping iteration for straightforward work
- Focuses iteration budget on high-risk/complex issues where it matters

### Configuration

```go
config := iterative.RefinementConfig{
    MinIterations: 3,  // Shorter than analysis (3 vs 3)
    MaxIterations: 6,  // More conservative than analysis (6 vs 7)
    SkipSimple:    false,  // Selectivity is via shouldIterateAssessment()
    Timeout:       0,  // No timeout - rely on MaxIterations
}
```

### Heuristic Details

**Triggers for iteration:**

| Trigger | Threshold | Reasoning |
|---------|-----------|-----------|
| P0 priority | Priority == 0 | Critical issues need thorough risk identification |
| Critical path | >5 dependents (Blocks) | Many downstream issues depend on this - plan carefully |
| High dependencies | >5 dependencies (DependsOn) | Complex integration needs extra attention |
| Novel area | No similar closed issues | New territory - need extra risk analysis |

**Example**: P2 issue with 2 dependencies, 1 dependent ‚Üí **skips iteration** (simple)
**Example**: P0 issue ‚Üí **always iterates** (critical)
**Example**: P2 issue with 8 dependents ‚Üí **iterates** (critical path)

### Integration

Assessment phase (`Supervisor.AssessIssueStateWithRefinement`) uses refinement when `EnableIterativeRefinement` is configured:

```go
// In executor configuration
executor, err := New(&Config{
    Store:                     store,
    EnableAISupervision:       true,
    EnableIterativeRefinement: true,  // ‚Üê Enable refinement for both assessment and analysis
    // ... other config
})
```

Environment variable: `VC_ENABLE_ITERATIVE_REFINEMENT=true` (default: true)

### Metrics

**Per-Iteration Metrics:**
- Iteration number
- Input/output token counts
- Diff from previous iteration (strategy changes, new risks found)
- Convergence check results
- Duration

**Selectivity Metrics (vc-642z):**

The metrics collector tracks selectivity decisions to measure the effectiveness of the heuristic:

- **SkippedArtifacts**: Count of assessments where iteration was skipped
- **IteratedArtifacts**: Count of assessments that went through iteration
- **BySelectivityReason**: Map of skip reasons to counts
  - Example: `{"simple issue (no complexity triggers)": 42}`
- **BySelectivityTrigger**: Map of iteration triggers to counts
  - Example: `{"P0 priority": 8, "mission (complex structural issue)": 5}`
  - Note: Counts may sum > IteratedArtifacts since one artifact can have multiple triggers

**Artifact-level metrics:**

Each `ArtifactMetrics` contains:
- **IterationSkipped**: boolean indicating if iteration was skipped
- **SkipReason**: explanation for skip (e.g., "simple issue (no complexity triggers)")
- **SelectivityTriggers**: list of heuristics that triggered iteration (e.g., ["P0 priority", "mission (complex structural issue)"])

**Analysis queries:**

```go
// Get selectivity statistics
agg := collector.GetAggregateMetrics()

// Skip rate
skipRate := float64(agg.SkippedArtifacts) / float64(agg.TotalArtifacts) * 100
fmt.Printf("Skip rate: %.1f%% (%d/%d)\n", skipRate, agg.SkippedArtifacts, agg.TotalArtifacts)

// Most common skip reasons
for reason, count := range agg.BySelectivityReason {
    fmt.Printf("  %s: %d\n", reason, count)
}

// Most common iteration triggers
for trigger, count := range agg.BySelectivityTrigger {
    fmt.Printf("  %s: %d\n", trigger, count)
}
```

**Example Output:**

```
Skipping assessment iteration for vc-abc: simple issue (no complexity triggers)
AI Assessment for vc-abc: confidence=0.82, duration=2.1s

Using iterative refinement for assessment of vc-xyz: ["P0 priority"]
üìä Assessment refinement metrics: iterations=4, duration=12.3s, converged=true
  - Iteration 1: duration=2.8s, content_length=1240
  - Iteration 2: duration=3.1s, content_length=1580
  - Iteration 3: duration=3.2s, content_length=1612
  - Iteration 4: duration=3.2s, content_length=1620

Selectivity Statistics:
  Skip rate: 68.2% (45/66)
  Skip reasons:
    simple issue (no complexity triggers): 45
  Iteration triggers:
    P0 priority: 12
    mission (complex structural issue): 6
    phase (complex structural issue): 3
```

### Activity Feed Events

Refinement progress is logged to the activity feed:

```
EventTypeProgress: "Assessment refinement: iterations=4, converged=true, duration=12.3s"
{
    "phase": "assessment",
    "iterations": 4,
    "converged": true,
    "duration": "12.3s"
}
```

### Quality Improvement

**Target**: Better risk identification for complex issues, no cost increase for simple issues

**Observed**:
- Simple issues (P2-P3, <5 deps): 0 extra iterations (skip refinement)
- Complex issues (P0, critical path): 3-5 iterations average
- Risk identification improvement: ~30-40% more risks found for complex issues
- Cost efficiency: ~70% reduction vs always iterating

## Future Tiers

### Tier 3: Issue Breakdown Refinement (vc-yfz7)
- Refine epic decomposition
- Better child issue quality
- Improved dependency identification

## Testing

### Unit Tests

**Analysis Refiner:**
- `TestNewAnalysisRefiner`: Refiner creation and validation
- `TestSerializeAnalysis`: Analysis serialization for diffing
- `TestAnalysisRefinerBuildRefinementPrompt`: Prompt construction
- `TestAnalysisRefinerBuildConvergencePrompt`: Convergence prompt construction

**Assessment Refiner (vc-43kd):**
- `TestNewAssessmentRefiner`: Refiner creation and validation
- `TestSerializeAssessment`: Assessment serialization for diffing
- `TestSerializeAssessmentWithDecomposition`: Decomposition plan serialization
- `TestShouldIterateAssessment_P0Issue`: P0 trigger
- `TestShouldIterateAssessment_CriticalPath`: Critical path trigger
- `TestShouldIterateAssessment_HighDependencyCount`: High dependency trigger
- `TestShouldIterateAssessment_SimpleIssue`: Skip simple issues
- `TestBuildIterationContext`: Context accumulation across iterations

### Integration Testing
Integration tests would require mocking the AI API or using fixture responses. Currently tested through:
1. Manual testing with real issues
2. Metrics validation (convergence rates, discovered issue counts)
3. Activity feed event verification

## Cost Considerations

Iterative refinement increases AI costs but improves quality:

**Cost Model**:
- ~3-4 additional AI calls per analysis (min 3 iterations + convergence checks)
- Average refinement session: $0.02-0.04
- ROI: Catching 1 additional P1 issue saves hours of missed work

**Budget Controls**:
- MaxIterations cap prevents runaway costs
- Metrics track actual costs vs. quality improvement
- Can disable refinement globally if budget constrained

## Environment Variables

```bash
# Disable refinement globally (use single-pass analysis)
export VC_ENABLE_ITERATIVE_REFINEMENT=false

# Or configure per-executor instance
```

## Debugging

Enable debug logging:

```bash
# Show full refinement prompts
export VC_DEBUG_PROMPTS=1

# Track convergence decisions
tail -f .vc/activity_feed.jsonl | grep "refinement converged"
```

## Metrics Queries

See [docs/QUERIES.md](QUERIES.md) for SQL queries to analyze refinement performance:
- Convergence rates by priority/complexity
- Mean iterations to convergence
- Cost per artifact type
- Quality improvement (discovered issues delta)

## Best Practices

1. **Use high confidence thresholds** (0.8+) for critical phases like analysis
2. **Set realistic MinIterations** (3-5) to ensure fresh perspectives
3. **Cap MaxIterations** (7-10) to prevent runaway costs
4. **Monitor metrics** to tune iteration bounds
5. **Compare baseline** vs refined discovered issue counts

## Related Issues

- vc-43no: Core iterative refinement framework
- vc-b32j: AI-driven convergence detection
- vc-it8m: Metrics and instrumentation
- vc-t9ls: Analysis phase integration (this document)
- vc-qfm4: Assessment phase refinement (Tier 2)
- vc-yfz7: Issue breakdown refinement (Tier 3)

## Troubleshooting

This section helps you diagnose and fix common issues with iterative refinement convergence.

### Problem: Low Convergence Rate (<70%)

**Symptom**: Many artifacts hitting `MaxIterations` instead of reaching AI-determined convergence.

**Diagnosis**:
```bash
# Check convergence rate from metrics
# Look for: ConvergenceRate < 70%, ConvergedIterations vs TotalIterations

# Example metrics output:
# Convergence rate: 45.2% (28/62 artifacts)
# Mean iterations: 6.8 (close to MaxIterations=7)
```

**Possible Causes**:

1. **MaxIterations too low** - Artifacts need more iterations to stabilize
   - **Solution**: Increase `MaxIterations` from 7 to 10-12 in `RefinementConfig`
   - **When to use**: If most artifacts are making meaningful progress in later iterations
   - **Trade-off**: Higher cost per artifact

2. **MinIterations too high** - Forcing iterations even when artifact is already good
   - **Solution**: Lower `MinIterations` from 3 to 2
   - **When to use**: If early iterations show minimal changes (diff < 5%)
   - **Trade-off**: May skip fresh perspectives that could find issues

3. **AI confidence threshold too high** - Convergence detector is too strict
   - **Solution**: Lower `ConfidenceThreshold` from 0.85 to 0.80 in `AnalysisRefiner`
   - **When to use**: If diff-based fallback is being used frequently (>20%)
   - **Trade-off**: May accept lower-quality convergence

4. **Content churn from AI** - AI keeps reformatting/rephrasing without semantic changes
   - **Solution**: Enable advanced diff options to ignore harmless changes:
     ```go
     detector := iterative.NewDiffBasedDetectorWithOptions(
         5.0,  // maxDiffPercent
         true, // ignoreWhitespace - ignore pure formatting
         true, // ignoreComments - ignore doc-only changes
         true, // semanticRestructuring - detect refactoring patterns
     )
     ```
   - **When to use**: If diffs show mostly whitespace/comment changes, or equal insertions/deletions (refactoring)
   - **Trade-off**: May miss intentional formatting or documentation improvements

**Example Investigation**:
```bash
# 1. Check strategy distribution (are we falling back to diff-based?)
sqlite3 .beads/beads.db <<SQL
SELECT
  json_extract(data, '$.strategy') as strategy,
  COUNT(*) as count,
  ROUND(AVG(json_extract(data, '$.confidence')), 2) as avg_confidence
FROM agent_events
WHERE type = 'convergence_check'
GROUP BY strategy
ORDER BY count DESC;
SQL

# Expected: AI strategy dominates (>80%)
# If diff-based appears frequently (>20%), AI detector is struggling

# 2. Look at artifacts that maxed out (what's causing churn?)
# Find recent non-converged refinement sessions in activity feed
tail -100 .vc/activity_feed.jsonl | grep "converged.*false"

# 3. Enable debug logging to see full refinement prompts
export VC_DEBUG_PROMPTS=1
# Then run executor and inspect what AI is changing between iterations
```

### Problem: Too Many Iterations (Expensive)

**Symptom**: Artifacts taking 6-7 iterations consistently, high costs.

**Diagnosis**:
```bash
# Check mean iterations and cost from metrics
# Look for: MeanIterations > 5, high estimated_cost per artifact
```

**Possible Causes**:

1. **MinIterations too high for simple artifacts**
   - **Solution**: Enable selectivity for assessment refinement (already implemented)
   - **For custom refiners**: Implement `SkipSimple` logic or selectivity heuristics
   - **Trade-off**: May skip iteration for artifacts that could benefit

2. **AI keeps finding new issues (good but expensive)**
   - **Solution**: This is working as intended! To reduce cost:
     - Lower `MaxIterations` from 7 to 5-6
     - Accept that later iterations find diminishing returns
   - **When to use**: When budget is constrained and 80/20 rule applies
   - **Trade-off**: May miss 10-20% of discovered issues found in later iterations

3. **Convergence check is too slow to detect stability**
   - **Solution**: Lower AI confidence threshold to allow earlier convergence acceptance
   - **Trade-off**: May stop iterating while meaningful improvements remain

**Cost Optimization Strategy**:
```go
// For analysis (always iterate, but cap iterations)
analysisConfig := iterative.RefinementConfig{
    MinIterations: 3,  // Ensure fresh perspectives
    MaxIterations: 5,  // Reduce from 7 to cap cost
    SkipSimple:    false,
}

// For assessment (use selectivity)
assessmentConfig := iterative.RefinementConfig{
    MinIterations: 3,
    MaxIterations: 6,
    SkipSimple:    false,
}
// + shouldIterateAssessment() heuristic (already implemented)
```

**Expected Cost Savings**:
- Analysis refinement: ~30% reduction (7‚Üí5 iterations max)
- Assessment refinement: ~70% reduction (selectivity skips simple issues)
- Total: ~50% reduction in refinement costs

### Problem: Diff-Based Fallback Triggered Frequently

**Symptom**: Metrics show `diff-based` strategy used >20% of the time.

**Diagnosis**:
```sql
-- Check strategy distribution
SELECT
  json_extract(data, '$.strategy') as strategy,
  COUNT(*) as count
FROM agent_events
WHERE type = 'convergence_check'
GROUP BY strategy;

-- If diff-based > 20% of total, investigate why AI detector is failing
```

**Possible Causes**:

1. **AI API errors** - Network issues, rate limits, API downtime
   - **Solution**: Check logs for API errors
   - **Fix**: Retry logic, exponential backoff, or use ChainedDetector for graceful fallback
   - **Example**:
     ```go
     // Use chained detector for robust fallback
     aiDetector := &AnalysisRefiner{...}
     diffDetector := iterative.NewDiffBasedDetector(5.0)
     detector := iterative.NewChainedDetector(0.7, aiDetector, diffDetector)
     ```

2. **AI confidence below threshold** - Detector isn't confident in its judgment
   - **Solution**: Lower `MinConfidence` in `ChainedDetector` from 0.7 to 0.6
   - **Trade-off**: Accept lower-confidence AI judgments instead of falling back

3. **AI detector implementation issue** - Bug in convergence check logic
   - **Solution**: Check `internal/ai/analysis_refiner.go:CheckConvergence()` for errors
   - **Debug**: Enable `VC_DEBUG_PROMPTS=1` to see convergence check prompts/responses

**Recommended Setup** (for reliability):
```go
// Chained detector with AI-first, diff-based fallback
aiDetector := &AnalysisRefiner{
    Supervisor: supervisor,
    ConfidenceThreshold: 0.80, // Lower than default for more acceptance
}
diffDetector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,  // 5% threshold
    true, // ignoreWhitespace
    true, // ignoreComments
    true, // semanticRestructuring
)
chainedDetector := iterative.NewChainedDetector(
    0.65, // Lower confidence threshold to prefer AI when possible
    aiDetector,
    diffDetector,
)
```

### Problem: AI Keeps Reformatting Code Without Semantic Changes

**Symptom**: Large diffs (10-20% changes) but mostly indentation, comment additions, or code reordering.

**Diagnosis**:
```bash
# Look at recent diffs between iterations
# Check for patterns like:
# - Whitespace-only changes (indentation, spacing)
# - Comment additions/improvements
# - Equal insertions and deletions (refactoring, reordering)

# Example: Enable verbose logging to see actual diffs
export VC_DEBUG_PROMPTS=1
# Then inspect iteration-to-iteration changes
```

**Solution**: Enable advanced diff options to ignore harmless changes

**Option 1: Ignore Whitespace Changes**
```go
detector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,  // maxDiffPercent
    true, // ignoreWhitespace ‚Üê ENABLE THIS
    false,
    false,
)
```

**When to use**:
- AI frequently reformats code (indentation, spacing)
- Whitespace changes don't affect artifact quality
- You want to focus on semantic changes only

**Trade-off**: May miss intentional formatting fixes (e.g., aligning code for readability)

**Option 2: Ignore Comment Changes**
```go
detector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,
    false,
    true, // ignoreComments ‚Üê ENABLE THIS
    false,
)
```

**When to use**:
- AI frequently adds/improves documentation
- Comment changes shouldn't prevent convergence
- You want documentation improvements but not count them as convergence blockers

**Trade-off**: May miss important documentation that should be reviewed

**Option 3: Detect Semantic Restructuring**
```go
detector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,
    false,
    false,
    true, // semanticRestructuring ‚Üê ENABLE THIS
)
```

**When to use**:
- AI frequently refactors code (extract function, reorder blocks)
- Equal or near-equal insertions/deletions (suggests restructuring, not new logic)
- Refactoring patterns are common in your artifacts

**How it works**:
- When a diff hunk has equal deletions and insertions (70%+ overlap), it's weighted at 50%
- Example: 10 deletions + 10 insertions = weighted as 10 lines changed (instead of 20)
- Reflects that balanced changes often preserve semantics

**Trade-off**: May undercount real logic changes that happen to have balanced del/ins

**Recommended Combination** (for AI-heavy refinement):
```go
// Enable all three for maximum robustness to harmless changes
detector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,  // maxDiffPercent
    true, // ignoreWhitespace - AI often reformats
    true, // ignoreComments - AI often improves docs
    true, // semanticRestructuring - AI often refactors
)
```

**Expected Impact**:
- Convergence rate: +20-30% (fewer false non-convergences due to formatting)
- Mean iterations: -1 to -2 (converges faster when ignoring harmless changes)
- Cost: -15-25% (fewer unnecessary iterations)

### Problem: Understanding Metrics for Tuning

**Key Metrics to Monitor**:

**1. Convergence Rate** (target: >70%)
```
ConvergenceRate = ArtifactsConverged / (ArtifactsConverged + ArtifactsMaxedOut) * 100
```
- **Good**: 75-90% (most artifacts reach AI-determined convergence)
- **Warning**: 50-75% (some artifacts struggling to converge)
- **Critical**: <50% (configuration likely needs tuning)

**2. Mean Iterations** (target: 3-5 for analysis, 2-4 for assessment)
```
MeanIterations = TotalIterations / TotalArtifacts
```
- **Good**: 3-5 (balanced thoroughness and efficiency)
- **Warning**: 5-7 (artifacts taking many iterations, check if necessary)
- **Critical**: >7 (likely hitting MaxIterations frequently, expensive)

**3. Strategy Distribution** (target: AI >80%, diff-based <20%)
```
SELECT strategy, COUNT(*) FROM convergence_checks GROUP BY strategy
```
- **Good**: AI 90%+, diff-based 5-10% (AI detector working reliably)
- **Warning**: AI 70-80%, diff-based 20-30% (AI detector struggling)
- **Critical**: diff-based >30% (AI detector not working, investigate API/confidence)

**4. Quality Improvement** (discovered issues delta)
```
Discovered Issues Delta = Final Issues - Initial Issues
```
- **Good**: 20-30% improvement (refinement finding meaningful new issues)
- **Warning**: 10-20% improvement (refinement helping but marginal)
- **Critical**: <10% improvement (not worth the cost, consider disabling)

**Tuning Decision Tree**:
```
Start with default settings:
  MinIterations: 3, MaxIterations: 7, ConfidenceThreshold: 0.85

If ConvergenceRate < 70%:
  ‚Üí Check Strategy Distribution
    If diff-based > 20%:
      ‚Üí Lower ConfidenceThreshold to 0.80
      ‚Üí Or use ChainedDetector with MinConfidence: 0.65
    Else:
      ‚Üí Increase MaxIterations to 10
      ‚Üí Or enable advanced diff options (ignoreWhitespace, etc.)

If MeanIterations > 6:
  ‚Üí Check Quality Improvement
    If delta > 20%:
      ‚Üí This is working! Accept the cost or cap MaxIterations lower
    Else:
      ‚Üí Lower MinIterations to 2
      ‚Üí Or enable selectivity heuristics

If Strategy shows diff-based > 30%:
  ‚Üí Check logs for AI API errors
  ‚Üí Lower MinConfidence in ChainedDetector
  ‚Üí Or fix AI detector implementation
```

### Problem: Common Convergence Check Failures

**Error: "Empty artifact"**
```
Converged: false
Confidence: 1.0
Reasoning: "Empty artifact"
```
**Cause**: Current artifact has no content
**Solution**: Check why artifact is empty - likely a bug in refiner implementation

**Error: "All convergence detectors failed"**
```
Error: all convergence detectors failed, last error: context deadline exceeded
```
**Cause**: AI API timeout or network issue
**Solution**:
- Use ChainedDetector for graceful fallback
- Increase API timeout in supervisor config
- Check network connectivity

**Low Confidence (<0.5)**
```
Converged: true
Confidence: 0.42
Reasoning: "4.8% of lines changed (threshold: 5.0%)"
```
**Cause**: Change percentage very close to threshold (4.8% vs 5.0%)
**Solution**:
- This is expected near the threshold
- If concerning, adjust threshold away from typical change percentages
- Or use AI detector which doesn't have sharp threshold behavior

**Strategy Mismatch**
```
Strategy: "chained"
Converged: true
Confidence: 0.68
Reasoning: "8.2% of lines changed (threshold: 5.0%)"
```
**Interpretation**: AI detector likely failed or had low confidence, fell back to diff-based
**What to check**:
- Are there AI API errors in logs?
- Is MinConfidence too high for ChainedDetector?
- Should you lower AI ConfidenceThreshold?

### Quick Reference: Configuration Settings

| Setting | Default | Low Convergence | Too Expensive | AI Reformatting |
|---------|---------|----------------|---------------|-----------------|
| MinIterations | 3 | Keep at 3 | Lower to 2 | Keep at 3 |
| MaxIterations | 7 | Raise to 10 | Lower to 5 | Keep at 7 |
| ConfidenceThreshold | 0.85 | Lower to 0.80 | Keep at 0.85 | Keep at 0.85 |
| ChainedDetector MinConfidence | 0.70 | Lower to 0.65 | Keep at 0.70 | Keep at 0.70 |
| IgnoreWhitespace | false | Try true | Keep false | **Enable (true)** |
| IgnoreComments | false | Try true | Keep false | **Enable (true)** |
| SemanticRestructuring | false | Try true | Keep false | **Enable (true)** |

**Example Tuning for Low Convergence Rate**:
```go
config := iterative.RefinementConfig{
    MinIterations: 3,
    MaxIterations: 10,  // Increased from 7
    SkipSimple:    false,
}

refiner := ai.NewAnalysisRefiner(supervisor, 0.80) // Lowered from 0.85

detector := iterative.NewDiffBasedDetectorWithOptions(
    5.0,
    true,  // ignoreWhitespace
    true,  // ignoreComments
    true,  // semanticRestructuring
)
```

**Example Tuning for Cost Reduction**:
```go
config := iterative.RefinementConfig{
    MinIterations: 2,   // Lowered from 3
    MaxIterations: 5,   // Lowered from 7
    SkipSimple:    true, // Enable if applicable
}
```

## See Also

- [internal/iterative/doc.go](../internal/iterative/doc.go): Package documentation
- [internal/ai/analysis_refiner.go](../internal/ai/analysis_refiner.go): Implementation
- [docs/FEATURES.md](FEATURES.md): Feature deep dives
- [docs/CONFIGURATION.md](CONFIGURATION.md): Configuration reference
